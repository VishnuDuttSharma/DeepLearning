{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through a basic understanding of the working of RNN. You can have a lok at __[Chris Olah's blog on RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)__ to understand RNN architecture.\n",
    "\n",
    "We will be using a simple RNNfor which the cell-equation (not a standard name) is:\n",
    "\n",
    "$h_t = f(X \\times W + h_{t-1} \\times U + b)$\n",
    "\n",
    "\n",
    "**Setup**:<br>\n",
    "Our aim in first problem is to predict the sum of 3 numbers with RNN.\n",
    "Thus for each input sequence $[x_0, x_1, x_2]$, output should be $y = x_0 + x_1 + x_2$\n",
    "\n",
    "**Note**: I know the same can be achieved with a simple neural net, but to keep it simple we are setting the problem this way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and model parameters\n",
    "seq_len = 3   #Length of each sequence \n",
    "rnn_size = 1  #Output shape of RNN\n",
    "input_size = 10000 #Numbers of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9],\n",
       "        [0],\n",
       "        [1]],\n",
       "\n",
       "       [[7],\n",
       "        [6],\n",
       "        [4]],\n",
       "\n",
       "       [[7],\n",
       "        [8],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [5],\n",
       "        [8]],\n",
       "\n",
       "       [[1],\n",
       "        [7],\n",
       "        [1]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat = np.random.randint(low=0, high=10, size=(input_size,3,1))\n",
    "all_feat[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [17],\n",
       "       [19],\n",
       "       [18],\n",
       "       [ 9]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label = np.apply_along_axis(func1d=np.sum, axis=1, arr=all_feat)\n",
    "all_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "Our model will have only a Simple RNN.<br> \n",
    "Our expectation with RNN is that it will learn to pass the input as it is to next layer.<br>\n",
    "One more thing to note: to keep things simple to understand, we'll use linear activation($y = f(x) = x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     (None, 3, 1)              0         \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3.0\n",
      "Trainable params: 3.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(3,1,), name='Input_Layer')\n",
    "y = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 9s - loss: 52.2708 - acc: 0.1204 - val_loss: 2.8959 - val_acc: 0.2050\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 8s - loss: 2.3057 - acc: 0.2139 - val_loss: 1.5666 - val_acc: 0.2775\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 8s - loss: 0.9501 - acc: 0.3466 - val_loss: 0.4068 - val_acc: 0.5105\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 9s - loss: 0.1705 - acc: 0.7825 - val_loss: 0.0324 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 9s - loss: 0.0084 - acc: 1.0000 - val_loss: 1.4700e-04 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=all_feat, y=all_label, batch_size=4, epochs=5, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model looks fine. Let's check few predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[[0]\n",
      "  [9]\n",
      "  [3]]\n",
      "\n",
      " [[4]\n",
      "  [2]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [5]\n",
      "  [6]]\n",
      "\n",
      " [[6]\n",
      "  [8]\n",
      "  [6]]\n",
      "\n",
      " [[6]\n",
      "  [6]\n",
      "  [6]]]\n",
      "\n",
      "Labels: \n",
      " [[12]\n",
      " [11]\n",
      " [12]\n",
      " [20]\n",
      " [18]]\n",
      "\n",
      "Predictions: \n",
      " [[ 12.00395012]\n",
      " [ 11.0082655 ]\n",
      " [ 12.00182343]\n",
      " [ 19.98966217]\n",
      " [ 17.99403191]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat[-5:,:])\n",
    "print('\\nLabels: \\n', all_label[-5:,:])\n",
    "print('\\nPredictions: \\n', model.predict(all_feat[-5:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what RNN learnt. A little info on the RNN weight matrices:<br>\n",
    "There are three weights:\n",
    "1. W: Input to RNN weight Matrix\n",
    "2. U: RNN to RNN (or hidden layer to RNN) weight Matrix\n",
    "3. b: Bias matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_lasyer = model.get_layer('RNN_Layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.99675155]], dtype=float32),\n",
       " array([[ 1.00106668]], dtype=float32),\n",
       " array([ 0.01110852], dtype=float32)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights match the expectations. RNN equation is:\n",
    "\n",
    "$h_t = f(X \\times W +  h_{t-1} \\times U + b)$\n",
    "\n",
    "As we have set $f$ to linear, the equations is\n",
    "\n",
    "$h_t = X \\times W +  h_{t-1} \\times U + b$\n",
    "\n",
    "We were expecting $W= 1, U = 1$ and $b = 0$, and the weights we got are quite close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to higher dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use one-hot encodings as the input to make the problem bit more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using keras preprocessing function\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat_feat = np.apply_along_axis(func1d=lambda x: to_categorical(x,10), arr=all_feat, axis=1)\n",
    "all_cat_feat = all_cat_feat.reshape(all_feat.shape[0], 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9],\n",
       "        [0],\n",
       "        [1]],\n",
       "\n",
       "       [[7],\n",
       "        [6],\n",
       "        [4]],\n",
       "\n",
       "       [[7],\n",
       "        [8],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [5],\n",
       "        [8]],\n",
       "\n",
       "       [[1],\n",
       "        [7],\n",
       "        [1]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]],\n",
       "\n",
       "       [[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cat_feat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating new model, we should delete the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     (None, 3, 10)             0         \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12.0\n",
      "Trainable params: 12.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(3,10,), name='Input_Layer')\n",
    "y = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "8000/8000 [==============================] - 19s - loss: 29.3267 - acc: 0.0964 - val_loss: 6.5214 - val_acc: 0.1465\n",
      "Epoch 2/8\n",
      "8000/8000 [==============================] - 17s - loss: 5.7982 - acc: 0.1399 - val_loss: 4.8307 - val_acc: 0.1525\n",
      "Epoch 3/8\n",
      "8000/8000 [==============================] - 16s - loss: 3.9756 - acc: 0.1681 - val_loss: 3.0087 - val_acc: 0.2085\n",
      "Epoch 4/8\n",
      "8000/8000 [==============================] - 15s - loss: 2.2039 - acc: 0.2259 - val_loss: 1.4164 - val_acc: 0.2835\n",
      "Epoch 5/8\n",
      "8000/8000 [==============================] - 15s - loss: 0.9000 - acc: 0.3513 - val_loss: 0.4745 - val_acc: 0.4915\n",
      "Epoch 6/8\n",
      "8000/8000 [==============================] - 16s - loss: 0.2291 - acc: 0.7034 - val_loss: 0.0760 - val_acc: 0.9335\n",
      "Epoch 7/8\n",
      "8000/8000 [==============================] - 16s - loss: 0.0273 - acc: 0.9932 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 8/8\n",
      "8000/8000 [==============================] - 18s - loss: 9.4697e-04 - acc: 1.0000 - val_loss: 3.0964e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(0.005), loss='mean_squared_error', metrics=['acc'])\n",
    "history = model.fit(x=all_cat_feat, y=all_label, batch_size=8, epochs=8, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo may have noticed that I chaanges training paramters like learning rate, batch size etc. This is done to reach high accuracy.\n",
    "\n",
    "Let's check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[[0]\n",
      "  [9]\n",
      "  [3]]\n",
      "\n",
      " [[4]\n",
      "  [2]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [5]\n",
      "  [6]]\n",
      "\n",
      " [[6]\n",
      "  [8]\n",
      "  [6]]\n",
      "\n",
      " [[6]\n",
      "  [6]\n",
      "  [6]]]\n",
      "\n",
      "Labels: \n",
      " [[12]\n",
      " [11]\n",
      " [12]\n",
      " [20]\n",
      " [18]]\n",
      "\n",
      "Predictions: \n",
      " [[ 11.99508286]\n",
      " [ 10.99825096]\n",
      " [ 11.99450302]\n",
      " [ 19.99518585]\n",
      " [ 17.99635315]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat[-5:,:])\n",
    "print('\\nLabels: \\n', all_label[-5:,:])\n",
    "print('\\nPredictions: \\n', model.predict(all_cat_feat[-5:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time input dimenion is 10 and output dimension is still 1. Looking back at RNN equation:\n",
    "\n",
    "$h_t = f(X \\times W +  h_{t-1} \\times U + b)$\n",
    "\n",
    "$W$ should have size $10 \\times 1$, while $U$ should still have size $1 \\times 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape:  (10, 1)\n",
      "U shape:  (1, 1)\n",
      "b shape:  (1,)\n"
     ]
    }
   ],
   "source": [
    "print('W shape: ', wgts_mats[0].shape)\n",
    "print('U shape: ', wgts_mats[1].shape)\n",
    "print('b shape: ', wgts_mats[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that W learns to transform one hot enocding to actual numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.94849992],\n",
       "        [-1.95010245],\n",
       "        [-0.95219231],\n",
       "        [ 0.046264  ],\n",
       "        [ 1.04455054],\n",
       "        [ 2.04350257],\n",
       "        [ 3.04185867],\n",
       "        [ 4.03992176],\n",
       "        [ 5.03859615],\n",
       "        [ 6.03598166]], dtype=float32),\n",
       " array([[ 1.00104976]], dtype=float32),\n",
       " array([ 2.95063329], dtype=float32)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U$ looks alright, but $W$ seems somewhat different. Let me add $b$ to $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W+b: \n",
      " [[  2.13336945e-03]\n",
      " [  1.00053084e+00]\n",
      " [  1.99844098e+00]\n",
      " [  2.99689722e+00]\n",
      " [  3.99518394e+00]\n",
      " [  4.99413586e+00]\n",
      " [  5.99249172e+00]\n",
      " [  6.99055481e+00]\n",
      " [  7.98922920e+00]\n",
      " [  8.98661518e+00]]\n",
      "\n",
      "U: \n",
      " [[ 1.00104976]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nW+b: \\n', wgts_mats[0]+wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a much, much clear understanding, round the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W+b: \n",
      " [[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]]\n",
      "\n",
      "U: \n",
      " [[ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nW+b: \\n', np.round(wgts_mats[0]+wgts_mats[2]))\n",
    "print('\\nU: \\n', np.round(wgts_mats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our input vector $X$, which has only one 1 at the position given by input number, is multipled with $W$, it essentially gives out the value at same positions from the weight matrix $W$.\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} 0 & 0 & 1 & 0 \\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \\\\ W_4 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0 \\times W_1 \\\\ + \\\\ 0 \\times W_2 \\\\ + \\\\ 1 \\times W_3 \\\\ + \\\\ 0 \\times W_4 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} W_3 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$\\begin{bmatrix} 0 & 0 & 1 & 0 \\end{bmatrix}$ is one-hot encoding for 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multitude of RNN models, you'll see embeddings beings used. Embedding are similar to one-hot encodings: An n-dimensional representation of your input(text generally) which learns the represetation along with the rest of the model.\n",
    "\n",
    "Here, We'll try to replace one-hot encodings with embeddings.\n",
    "\n",
    "Input will be numbers, need to be reshaped, and before the RNN layer, there will be an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat_reshaped = all_feat.reshape(all_feat.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 3, 10)             100       \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 112.0\n",
      "Trainable params: 112.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_1 = Input(shape=(3,), name='Input_Layer')\n",
    "x = Embedding(input_dim=10, output_dim=10, name='Embedding_Layer')(input_1)\n",
    "y = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 5s - loss: 4.7818 - acc: 0.7199 - val_loss: 2.5118e-08 - val_acc: 1.0000\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 4s - loss: 7.0748e-10 - acc: 1.0000 - val_loss: 8.6389e-12 - val_acc: 1.0000\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 4s - loss: 4.5370e-12 - acc: 1.0000 - val_loss: 2.4172e-12 - val_acc: 1.0000\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 5s - loss: 1.2825e-12 - acc: 1.0000 - val_loss: 9.2436e-13 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(0.01), loss='mean_squared_error', metrics=['acc'])\n",
    "history = model.fit(x=all_feat_reshaped, y=all_label, batch_size=8, epochs=4, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[0 9 3]\n",
      " [4 2 5]\n",
      " [1 5 6]\n",
      " [6 8 6]\n",
      " [6 6 6]]\n",
      "\n",
      "Labels: \n",
      " [[12]\n",
      " [11]\n",
      " [12]\n",
      " [20]\n",
      " [18]]\n",
      "\n",
      "Predictions: \n",
      " [[ 11.99999905]\n",
      " [ 11.        ]\n",
      " [ 12.        ]\n",
      " [ 20.00000191]\n",
      " [ 18.00000381]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat_reshaped[-5:,:])\n",
    "print('\\nLabels: \\n', all_label[-5:,:])\n",
    "print('\\nPredictions: \\n', model.predict(all_feat_reshaped[-5:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we need to check embedding weight too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_layer = model.get_layer('Embedding_Layer')\n",
    "embd_mats = embd_layer.get_weights()\n",
    "\n",
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer should have size = $10 \\times 10$, as we're mapping 10 numbers(integers to be precise) to 10 dimensional vectors (1 vector for each of the number). In the weight matrix, index indicates the integer to which it is mapped.\n",
    "\n",
    "RNN weight shapes will be similar to the previous excerxise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding W shape:  (10, 10)\n",
      "W shape:  (10, 1)\n",
      "U shape:  (1, 1)\n",
      "b shape:  (1,)\n"
     ]
    }
   ],
   "source": [
    "print('Embedding W shape: ', embd_mats[0].shape)\n",
    "print('W shape: ', wgts_mats[0].shape)\n",
    "print('U shape: ', wgts_mats[1].shape)\n",
    "print('b shape: ', wgts_mats[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.06210777, -0.02745032,  0.03699404, -0.04357917,  0.00985156,\n",
       "          0.05047535, -0.07252501, -0.0060824 ,  0.08501084,  0.02329089],\n",
       "        [-0.03501924,  0.05478969, -0.06651403,  0.0606865 ,  0.07692657,\n",
       "         -0.0303007 ,  0.10046678,  0.02375769, -0.00521658, -0.03262439],\n",
       "        [-0.12506177,  0.09839212, -0.17181483,  0.16981345,  0.16977352,\n",
       "         -0.08540933,  0.16722172,  0.15118837, -0.1214526 , -0.10981815],\n",
       "        [-0.17968415,  0.25980386, -0.22894789,  0.24273922,  0.26052341,\n",
       "         -0.23109256,  0.2227577 ,  0.22931208, -0.18935528, -0.25136626],\n",
       "        [-0.3381401 ,  0.28742275, -0.3784467 ,  0.29970467,  0.29632148,\n",
       "         -0.36220279,  0.33802927,  0.28446689, -0.31542966, -0.29835254],\n",
       "        [-0.38825303,  0.43541983, -0.4055247 ,  0.43372044,  0.34076664,\n",
       "         -0.40598577,  0.42293841,  0.41570613, -0.45533296, -0.40606618],\n",
       "        [-0.46653211,  0.52266681, -0.48973432,  0.48285624,  0.50394773,\n",
       "         -0.64239901,  0.46153784,  0.47139424, -0.55294889, -0.45976666],\n",
       "        [-0.57181919,  0.57065916, -0.51719308,  0.57912141,  0.53203046,\n",
       "         -0.73055142,  0.54653585,  0.59713608, -0.725555  , -0.61746806],\n",
       "        [-0.69791192,  0.67029899, -0.62343609,  0.66363454,  0.69465142,\n",
       "         -0.7854681 ,  0.624156  ,  0.65458065, -0.76210004, -0.65989387],\n",
       "        [-0.71957815,  0.75552607, -0.76832122,  0.75740767,  0.68210703,\n",
       "         -0.97855085,  0.67297399,  0.76844192, -0.93439114, -0.77425069]], dtype=float32)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.29411161],\n",
       "        [ 1.12959146],\n",
       "        [-1.28118789],\n",
       "        [ 1.07222223],\n",
       "        [ 1.29417169],\n",
       "        [-0.73790312],\n",
       "        [ 1.29149568],\n",
       "        [ 1.28831363],\n",
       "        [-0.79495221],\n",
       "        [-1.01940799]], dtype=float32),\n",
       " array([[ 1.00000012]], dtype=float32),\n",
       " array([ 0.42282528], dtype=float32)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only $U$ makes the sense. Remember the RNN equaition:\n",
    "\n",
    "$h_t = f(X \\times W +  h_{t-1} \\times U + b)$\n",
    "\n",
    "Here, $X$ is the embedding output. Let's do one more transformation:  $ W_{embd} \\times W + b$, this will give us a number a vector containing 10 numbers, each corresponding to input number.\n",
    "\n",
    "Let's do it one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42282546],\n",
       "       [ 0.5771746 ],\n",
       "       [ 1.57717419],\n",
       "       [ 2.57717466],\n",
       "       [ 3.57717443],\n",
       "       [ 4.57717419],\n",
       "       [ 5.57717419],\n",
       "       [ 6.57717371],\n",
       "       [ 7.57717419],\n",
       "       [ 8.57717419]], dtype=float32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(embd_mats[0], wgts_mats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.78813934e-07],\n",
       "       [  9.99999881e-01],\n",
       "       [  1.99999952e+00],\n",
       "       [  3.00000000e+00],\n",
       "       [  3.99999976e+00],\n",
       "       [  4.99999952e+00],\n",
       "       [  5.99999952e+00],\n",
       "       [  6.99999905e+00],\n",
       "       [  7.99999952e+00],\n",
       "       [  8.99999905e+00]], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " W_embd * W + b: \n",
      " [[ -1.78813934e-07]\n",
      " [  9.99999881e-01]\n",
      " [  1.99999952e+00]\n",
      " [  3.00000000e+00]\n",
      " [  3.99999976e+00]\n",
      " [  4.99999952e+00]\n",
      " [  5.99999952e+00]\n",
      " [  6.99999905e+00]\n",
      " [  7.99999952e+00]\n",
      " [  8.99999905e+00]]\n",
      "\n",
      "U: \n",
      " [[ 1.00000012]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n W_embd * W + b: \\n', np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes some sense, right!\n",
    "\n",
    "Let's round it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " W_embd * W + b: \n",
      " [[-0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]]\n",
      "\n",
      "U: \n",
      " [[ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n W_embd * W + b: \\n', np.round(np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2]))\n",
    "print('\\nU: \\n', np.round(wgts_mats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of what happened:\n",
    "\n",
    "When you input an integer to ebmedding layer,it gives out a vector at corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06210777, -0.02745032,  0.03699404, -0.04357917,  0.00985156,\n",
       "         0.05047535, -0.07252501, -0.0060824 ,  0.08501084,  0.02329089],\n",
       "       [-0.03501924,  0.05478969, -0.06651403,  0.0606865 ,  0.07692657,\n",
       "        -0.0303007 ,  0.10046678,  0.02375769, -0.00521658, -0.03262439],\n",
       "       [-0.12506177,  0.09839212, -0.17181483,  0.16981345,  0.16977352,\n",
       "        -0.08540933,  0.16722172,  0.15118837, -0.1214526 , -0.10981815],\n",
       "       [-0.17968415,  0.25980386, -0.22894789,  0.24273922,  0.26052341,\n",
       "        -0.23109256,  0.2227577 ,  0.22931208, -0.18935528, -0.25136626],\n",
       "       [-0.3381401 ,  0.28742275, -0.3784467 ,  0.29970467,  0.29632148,\n",
       "        -0.36220279,  0.33802927,  0.28446689, -0.31542966, -0.29835254],\n",
       "       [-0.38825303,  0.43541983, -0.4055247 ,  0.43372044,  0.34076664,\n",
       "        -0.40598577,  0.42293841,  0.41570613, -0.45533296, -0.40606618],\n",
       "       [-0.46653211,  0.52266681, -0.48973432,  0.48285624,  0.50394773,\n",
       "        -0.64239901,  0.46153784,  0.47139424, -0.55294889, -0.45976666],\n",
       "       [-0.57181919,  0.57065916, -0.51719308,  0.57912141,  0.53203046,\n",
       "        -0.73055142,  0.54653585,  0.59713608, -0.725555  , -0.61746806],\n",
       "       [-0.69791192,  0.67029899, -0.62343609,  0.66363454,  0.69465142,\n",
       "        -0.7854681 ,  0.624156  ,  0.65458065, -0.76210004, -0.65989387],\n",
       "       [-0.71957815,  0.75552607, -0.76832122,  0.75740767,  0.68210703,\n",
       "        -0.97855085,  0.67297399,  0.76844192, -0.93439114, -0.77425069]], dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_mats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38825303,  0.43541983, -0.4055247 ,  0.43372044,  0.34076664,\n",
       "       -0.40598577,  0.42293841,  0.41570613, -0.45533296, -0.40606618], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In input was '5', output will be\n",
    "embd_mats[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This input is similar to one-hot encoding. \n",
    "\n",
    "In the next step(RNN), this vector get multipled to $W$ to produce a vector of rnn_size, which in this case is 1, so it gives out one number in our case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you could see, embeddings learn represetation in combination to other matrices and thus might be difficult to explain directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
