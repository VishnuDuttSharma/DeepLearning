{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is next part of blog post at [Medium (Part 1)](https://medium.com/@vishnusharma619/understanding-what-rnn-learns-part-15f1b23b5f7b4) and notebook at [Part 1 notebook](https://github.com/VishnuDuttSharma/DeepLearning/blob/master/experiments/RNN_Explain.ipynb). Please make sure that you have gone through them as I will be using some concept from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I will focus more on embeddings as the concept seems obscure to beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last part, we have seen that we can use embeddings to learn representation for input. This concept will get clearer in this part.\n",
    "\n",
    "**Setup:**<br>\n",
    "In this part, our aim is to pedict whether a given sequence of numbers contains '4' or not. This is a classification problem. You can draw parallels with a problem where your aim is to find whether the sentence contains the word 'not' in it(a very basic form of sentiment analysis).\n",
    "\n",
    "\n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, SimpleRNN, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and model parameters\n",
    "seq_len = 3   #Length of each sequence \n",
    "rnn_size = 1  #Output shape of RNN\n",
    "input_size = 10000 #Numbers of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat = np.random.randint(low=0, high=10, size=(input_size,3))\n",
    "all_label = np.apply_along_axis(func1d=lambda x: int(np.any(x==4)), axis=1, arr=all_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Sample:\n",
      " [[8 8 6]\n",
      " [8 5 5]\n",
      " [7 1 4]\n",
      " [7 1 0]\n",
      " [7 8 5]]\n",
      "\n",
      "Output Sample:\n",
      " [0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput Sample:\\n', all_feat[:5])\n",
    "print('\\nOutput Sample:\\n',all_label[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we are again going to choose embedding size=10, as we wish to use embeddings as a replacement to one-hot encoding.\n",
    "\n",
    "But we can't directly use previous model(which solves a regression problem). This time we want to predict probability. So after our RNN, we'll use a sigmoid activation.\n",
    "\n",
    "$y = sigmoid(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png)\n",
    "\n",
    "As you can see, sigmoid squashes output to a number between 0-1, such that higher the number, closer it is to 1. Similary, lower the number,closer it is to 0.\n",
    "\n",
    "We will use binary crossentropy as the loss, which is generally used for classification problems.  \n",
    "\n",
    "## Classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 3, 10)             100       \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 12        \n",
      "_________________________________________________________________\n",
      "Activation_Layer (Activation (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 112.0\n",
      "Trainable params: 112.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_1 = Input(shape=(3,), name='Input_Layer')\n",
    "x = Embedding(input_dim=10, output_dim=10, name='Embedding_Layer')(input_1)\n",
    "x = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "y = Activation('sigmoid', name='Activation_Layer')(x)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 4s - loss: 0.0106 - acc: 0.9956 - val_loss: 6.5674e-05 - val_acc: 1.0000\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 4s - loss: 3.2502e-05 - acc: 1.0000 - val_loss: 1.6959e-05 - val_acc: 1.0000\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 4s - loss: 1.0502e-05 - acc: 1.0000 - val_loss: 6.8796e-06 - val_acc: 1.0000\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 4s - loss: 4.6197e-06 - acc: 1.0000 - val_loss: 3.3005e-06 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(0.02), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x=all_feat, y=all_label, batch_size=8, epochs=4, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[5 6 9]\n",
      " [9 9 6]\n",
      " [5 8 4]\n",
      " [5 8 1]\n",
      " [6 8 1]\n",
      " [9 2 7]\n",
      " [6 0 0]\n",
      " [9 7 8]\n",
      " [8 7 9]\n",
      " [0 2 7]]\n",
      "\n",
      "Labels: \n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      "\n",
      "Predictions: \n",
      " [[  2.12621512e-06]\n",
      " [  2.11419456e-06]\n",
      " [  9.99977827e-01]\n",
      " [  1.95994267e-06]\n",
      " [  2.24871815e-06]\n",
      " [  2.05333026e-06]\n",
      " [  2.08695269e-06]\n",
      " [  1.90066737e-06]\n",
      " [  1.92356879e-06]\n",
      " [  1.98162775e-06]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat[-10:,:])\n",
    "print('\\nLabels: \\n', all_label[-10:])\n",
    "print('\\nPredictions: \\n', model.predict(all_feat[-10:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_layer = model.get_layer('Embedding_Layer')\n",
    "embd_mats = embd_layer.get_weights()\n",
    "\n",
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if we got expected shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding W shape:  (10, 10)\n",
      "W shape:  (10, 1)\n",
      "U shape:  (1, 1)\n",
      "b shape:  (1,)\n"
     ]
    }
   ],
   "source": [
    "print('Embedding W shape: ', embd_mats[0].shape)\n",
    "print('W shape: ', wgts_mats[0].shape)\n",
    "print('U shape: ', wgts_mats[1].shape)\n",
    "print('b shape: ', wgts_mats[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at the weights, let's see what are our expectations from them:<br>\n",
    "$h_t = f(X \\times W + h_{t-1} \\times U + b)$\n",
    "Reminder: f is linear.\n",
    "\n",
    "We would expect that combination of embedding matrix and W has a high value for position corresponding to input=4. U should simply forward the output to next cell.\n",
    "\n",
    "This way, if model sees 4 anywhere, output will become high. U will help carry this score to next cell.\n",
    "\n",
    "\n",
    "Embedding weights matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.14044982,  0.21658441, -0.22791751,  0.21502978, -0.19579233,\n",
       "          0.18623219,  0.2239477 , -0.14737135,  0.1834836 ,  0.17848013],\n",
       "        [-0.16119297,  0.15270022, -0.22112088,  0.1870423 , -0.21034855,\n",
       "          0.22094215,  0.22617932, -0.19690502,  0.16201828,  0.16872635],\n",
       "        [-0.20279713,  0.17129959, -0.18022029,  0.17888239, -0.19878508,\n",
       "          0.18757217,  0.18948197, -0.15959248,  0.21192279,  0.1793922 ],\n",
       "        [-0.16744758,  0.18689834, -0.20116815,  0.16811925, -0.21271777,\n",
       "          0.16176091,  0.23074993, -0.22508025,  0.17736089,  0.19697021],\n",
       "        [ 0.87358165, -0.92919093,  1.27148986, -0.97707129,  1.06918406,\n",
       "         -1.25646746, -1.07512939,  1.01008677, -1.22653592, -1.07279408],\n",
       "        [-0.20252103,  0.13714807, -0.17961763,  0.20893431, -0.21239041,\n",
       "          0.19352351,  0.20141117, -0.19705266,  0.16560002,  0.20322703],\n",
       "        [-0.20628117,  0.14427678, -0.21260698,  0.15537232, -0.14697219,\n",
       "          0.18460469,  0.15442781, -0.18720369,  0.2582643 ,  0.21449965],\n",
       "        [-0.15330791,  0.21459399, -0.22178707,  0.13402544, -0.13827942,\n",
       "          0.2318393 ,  0.21597138, -0.20058507,  0.23698436,  0.19769941],\n",
       "        [-0.15754659,  0.1512261 , -0.20155624,  0.22907215, -0.17636189,\n",
       "          0.18587922,  0.17176367, -0.19894339,  0.22978529,  0.18345623],\n",
       "        [-0.13863607,  0.22530088, -0.15114433,  0.19402944, -0.21190034,\n",
       "          0.1904562 ,  0.22017194, -0.18479121,  0.2103454 ,  0.18715787]], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't give a very clear picture, right. <br>\n",
    "Let's check RNN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.06557703],\n",
       "        [-1.83462012],\n",
       "        [ 1.95859563],\n",
       "        [-2.21554518],\n",
       "        [ 1.99453723],\n",
       "        [-2.04525447],\n",
       "        [-1.56783688],\n",
       "        [ 1.71975875],\n",
       "        [-1.72644722],\n",
       "        [-1.72969031]], dtype=float32),\n",
       " array([[ 1.21534526]], dtype=float32),\n",
       " array([ 0.02591785], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U seems fine. W looks somewhat regular. Let's check our regular construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " W_embd * W + b: \n",
      " [[ -3.58580279]\n",
      " [ -3.57090545]\n",
      " [ -3.48436666]\n",
      " [ -3.58020806]\n",
      " [ 20.28836632]\n",
      " [ -3.57022905]\n",
      " [ -3.47717571]\n",
      " [ -3.60041976]\n",
      " [ -3.53663325]\n",
      " [ -3.56173849]]\n",
      "\n",
      "U: \n",
      " [[ 1.21534526]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n W_embd * W + b: \\n', np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At index 4, we get a high positive value, while others are negative. This way if 4 is encontered in sequence, model creates high positive output, otherwise a negative value. \n",
    "\n",
    "U is slightly higher than 1, thus the previous score with get amplified (i.e. positive input will give higher positive value, negative input will give more negative value)\n",
    "\n",
    "Makes sense, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower embedding size\n",
    "\n",
    "Embeddings are supposed to learn representations, right? Let's try lower embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "\n",
    "\n",
    "input_1 = Input(shape=(3,), name='Input_Layer')\n",
    "x = Embedding(input_dim=10, output_dim=4, name='Embedding_Layer')(input_1)\n",
    "x = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "y = Activation('sigmoid', name='Activation_Layer')(x)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 4s - loss: 0.0180 - acc: 0.9916 - val_loss: 1.4316e-04 - val_acc: 1.0000\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 4s - loss: 6.9616e-05 - acc: 1.0000 - val_loss: 3.6003e-05 - val_acc: 1.0000\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 4s - loss: 2.2050e-05 - acc: 1.0000 - val_loss: 1.4364e-05 - val_acc: 1.0000\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 4s - loss: 9.6034e-06 - acc: 1.0000 - val_loss: 6.8176e-06 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(0.02), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x=all_feat, y=all_label, batch_size=8, epochs=4, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[5 6 9]\n",
      " [9 9 6]\n",
      " [5 8 4]\n",
      " [5 8 1]\n",
      " [6 8 1]\n",
      " [9 2 7]\n",
      " [6 0 0]\n",
      " [9 7 8]\n",
      " [8 7 9]\n",
      " [0 2 7]]\n",
      "\n",
      "Labels: \n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      "\n",
      "Predictions: \n",
      " [[  4.59608827e-06]\n",
      " [  4.54348674e-06]\n",
      " [  9.99956727e-01]\n",
      " [  4.26225279e-06]\n",
      " [  4.60571482e-06]\n",
      " [  4.42924829e-06]\n",
      " [  4.36698019e-06]\n",
      " [  4.22405401e-06]\n",
      " [  4.26433053e-06]\n",
      " [  4.23319989e-06]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat[-10:,:])\n",
    "print('\\nLabels: \\n', all_label[-10:])\n",
    "print('\\nPredictions: \\n', model.predict(all_feat[-10:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_layer = model.get_layer('Embedding_Layer')\n",
    "embd_mats = embd_layer.get_weights()\n",
    "\n",
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.28512904,  0.25177249, -0.27367339,  0.30789083],\n",
       "        [ 0.25492424,  0.26012757, -0.29675287,  0.30629158],\n",
       "        [ 0.28270748,  0.28677672, -0.26648894,  0.25294301],\n",
       "        [ 0.29766184,  0.26947719, -0.26931292,  0.28334641],\n",
       "        [-1.50977004, -1.4418962 ,  1.47527599, -1.76327348],\n",
       "        [ 0.33177933,  0.24144135, -0.26087469,  0.27504072],\n",
       "        [ 0.25945908,  0.24145941, -0.25259978,  0.34160569],\n",
       "        [ 0.29079974,  0.26891741, -0.2553148 ,  0.30429697],\n",
       "        [ 0.29276261,  0.23101816, -0.28032303,  0.29925823],\n",
       "        [ 0.27864024,  0.23305847, -0.24671097,  0.35506517]], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time embeddings seem more intuitive. We can see bigger and negative numbers at index 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " W_embd * W + b: \n",
      " [[ -3.31143618]\n",
      " [ -3.31706262]\n",
      " [ -3.2351234 ]\n",
      " [ -3.3211081 ]\n",
      " [ 19.09644699]\n",
      " [ -3.28206587]\n",
      " [ -3.23133922]\n",
      " [ -3.31450295]\n",
      " [ -3.26382184]\n",
      " [ -3.28180361]]\n",
      "\n",
      "U: \n",
      " [[ 1.23604953]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n W_embd * W + b: \\n', np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks similar to previous model. So lower embedding works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal embedding\n",
    "\n",
    "Let's get more intuitive. It's a classification problem. So why we need 10 or 4 embedding. We should learn only 1 number: hige value for 4, low value for others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 3, 1)              10        \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "Activation_Layer (Activation (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 13.0\n",
      "Trainable params: 13.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 4s - loss: 0.0248 - acc: 0.9902 - val_loss: 4.7527e-04 - val_acc: 1.0000\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 4s - loss: 2.2767e-04 - acc: 1.0000 - val_loss: 1.1516e-04 - val_acc: 1.0000\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 4s - loss: 6.9932e-05 - acc: 1.0000 - val_loss: 4.4882e-05 - val_acc: 1.0000\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 4s - loss: 2.9992e-05 - acc: 1.0000 - val_loss: 2.1167e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "\n",
    "input_1 = Input(shape=(3,), name='Input_Layer')\n",
    "x = Embedding(input_dim=10, output_dim=1, name='Embedding_Layer')(input_1)\n",
    "x = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "y = Activation('sigmoid', name='Activation_Layer')(x)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=y)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=Adam(0.02), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x=all_feat, y=all_label, batch_size=8, epochs=4, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding weights: \n",
      " [[-0.43760461]\n",
      " [-0.43051854]\n",
      " [-0.43027946]\n",
      " [-0.44076917]\n",
      " [ 2.7104342 ]\n",
      " [-0.44030327]\n",
      " [-0.42853352]\n",
      " [-0.43567708]\n",
      " [-0.43628559]\n",
      " [-0.43209222]]\n",
      "\n",
      "RNN weights: \n",
      " [[ 6.39243031]] [[ 1.37941635]] [ 0.15867162]\n",
      "\n",
      " W_embd * W + b: \n",
      " [[ -2.63868523]\n",
      " [ -2.59338808]\n",
      " [ -2.59185982]\n",
      " [ -2.65891457]\n",
      " [ 17.48493385]\n",
      " [ -2.65593624]\n",
      " [ -2.58069897]\n",
      " [ -2.62636375]\n",
      " [ -2.63025355]\n",
      " [ -2.60344768]]\n",
      "\n",
      "U: \n",
      " [[ 1.37941635]]\n"
     ]
    }
   ],
   "source": [
    "embd_layer = model.get_layer('Embedding_Layer')\n",
    "embd_mats = embd_layer.get_weights()\n",
    "\n",
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()\n",
    "\n",
    "print('\\nEmbedding weights: \\n',embd_mats[0])\n",
    "print('\\nRNN weights: \\n', wgts_mats[0], wgts_mats[1], wgts_mats[2] )\n",
    "\n",
    "print('\\n W_embd * W + b: \\n', np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can observe that both embedding and RNN weights look sensible. Transformed weights are still similar to previous models. \n",
    "\n",
    "\n",
    "### Lesson:\n",
    "When choosing embedding size,go with your the number that you think should be enough to capture number of feature required to help model reach a decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting closer to real-world models\n",
    "\n",
    "We have consistently been using *Linear* activation i.e. no change in input to make things easier to understand. This approach will fail for longer sequences. Why?\n",
    "\n",
    "Remember the value of $U$; it was somewhat higher than 1. So a big value coming from the start of sequence will keep getting multipled by $U$ causing a blowup. A low value at the start will vanish.<br>\n",
    "If the value of $U$ < 1, the the reverse will happen.\n",
    "\n",
    "Consider different complex variations of input over longer sequences and you will get the idea of the problems.\n",
    "\n",
    "To deal with it we will use a non-activation. Genrally it's *tanh*, but as I want my probability, I will use *sigmoid* (*tanh* output number in range (-1,1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 3, 1)              10        \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 13.0\n",
      "Trainable params: 13.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 5s - loss: 0.1316 - acc: 0.9599 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 4s - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 4s - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 4s - loss: 9.8492e-04 - acc: 1.0000 - val_loss: 6.7974e-04 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "\n",
    "input_1 = Input(shape=(3,), name='Input_Layer')\n",
    "x = Embedding(input_dim=10, output_dim=1, name='Embedding_Layer')(input_1)\n",
    "y = SimpleRNN(rnn_size, activation='sigmoid', name='RNN_Layer')(x)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=y)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=Adam(0.03), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x=all_feat, y=all_label, batch_size=8, epochs=4, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[5 6 9]\n",
      " [9 9 6]\n",
      " [5 8 4]\n",
      " [5 8 1]\n",
      " [6 8 1]\n",
      " [9 2 7]\n",
      " [6 0 0]\n",
      " [9 7 8]\n",
      " [8 7 9]\n",
      " [0 2 7]]\n",
      "\n",
      "Labels: \n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      "\n",
      "Predictions: \n",
      " [[  4.83995711e-04]\n",
      " [  5.38199791e-04]\n",
      " [  9.99984384e-01]\n",
      " [  5.32271166e-04]\n",
      " [  5.32272446e-04]\n",
      " [  5.16529719e-04]\n",
      " [  4.35839931e-04]\n",
      " [  5.51217643e-04]\n",
      " [  4.83845797e-04]\n",
      " [  5.16526750e-04]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat[-10:,:])\n",
    "print('\\nLabels: \\n', all_label[-10:])\n",
    "print('\\nPredictions: \\n', model.predict(all_feat[-10:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding weights: \n",
      " [[ 1.22580421]\n",
      " [ 1.1842196 ]\n",
      " [ 1.15448904]\n",
      " [ 1.20168781]\n",
      " [-2.71523046]\n",
      " [ 1.19090605]\n",
      " [ 1.18169999]\n",
      " [ 1.19069457]\n",
      " [ 1.17678034]\n",
      " [ 1.20412242]]\n",
      "\n",
      "RNN weights: \n",
      " [[-4.77058172]] [[ 13.87013912]] [-1.89605367]\n",
      "\n",
      " W_embd * W + b: \n",
      " [[ -7.74385309]\n",
      " [ -7.54547024]\n",
      " [ -7.40363789]\n",
      " [ -7.62880373]\n",
      " [ 11.05717564]\n",
      " [ -7.57736826]\n",
      " [ -7.53345013]\n",
      " [ -7.57635975]\n",
      " [ -7.50998068]\n",
      " [ -7.64041805]]\n",
      "\n",
      "U: \n",
      " [[ 13.87013912]]\n"
     ]
    }
   ],
   "source": [
    "embd_layer = model.get_layer('Embedding_Layer')\n",
    "embd_mats = embd_layer.get_weights()\n",
    "\n",
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()\n",
    "\n",
    "print('\\nEmbedding weights: \\n',embd_mats[0])\n",
    "print('\\nRNN weights: \\n', wgts_mats[0], wgts_mats[1], wgts_mats[2] )\n",
    "\n",
    "print('\\n W_embd * W + b: \\n', np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights still make sense. We can use non-linear activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
