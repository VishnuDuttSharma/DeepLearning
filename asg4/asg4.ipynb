{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import ptb_reader\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "dataset_size = 16000\n",
    "num_steps = 20\n",
    "hidden_size_1 = 128\n",
    "# hidden_size_2 = 64\n",
    "feat_len = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_loss(y_true, y_pred):\n",
    "    return (K.mean((K.categorical_crossentropy(y_pred, y_true)),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_acc(y_true, y_pred):\n",
    "    cross_ent = seq_loss(y_true, y_pred)\n",
    "    return K.pow(cross_ent,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x, y):\n",
    "    xmat = np.zeros((x.shape[0], num_steps, feat_len))\n",
    "    for j in range(x.shape[1]):\n",
    "        for i in zip(range(num_steps), x[j]):\n",
    "            xmat[j, i[0], i[1]] = 1\n",
    "    \n",
    "    ymat = np.zeros((x.shape[0], feat_len))\n",
    "    for j in range(x.shape[0]):\n",
    "        ymat[j, y[j]] = 1\n",
    "    return xmat, ymat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "#     model.add(Embedding( output_dim=500, input_dim=feat_len, input_length=num_steps))\n",
    "    model.add(Embedding( output_dim=500,input_dim=1, input_length=num_steps))\n",
    "#     model.add(Embedding( num_steps, feat_len))\n",
    "    # , dropout=0.2, recurrent_dropout=0.2\n",
    "    model.add(LSTM(hidden_size_1, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "#     model.add(LSTM(hidden_size_2, return_sequences=True))\n",
    "#     model.add(Dense(feat_len*num_steps, activation='softmax'))\n",
    "    model.add(TimeDistributed(Dense(units = feat_len, input_dim = hidden_size_1, activation = 'softmax'))) \n",
    "    \n",
    "    adam = Adam(lr=0.05, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=seq_loss,\n",
    "              optimizer='adam',\n",
    "              metrics=[ seq_acc])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.add(Embedding(max_features, num_steps))\n",
    "# model.add(LSTM(50, input_shape=(1,1), dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(vocab, activation='softmax'))\n",
    "# model.pop()\n",
    "# model.add(Dense(10, input_shape=(10,), activation='softmax'))\n",
    "\n",
    "# model.pop()\n",
    "# model.pop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    word_to_id = ptb_reader._build_vocab(filepath)\n",
    "    return ptb_reader._file_to_word_ids(filepath, word_to_id), len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_data, verbose = 0, model = None):\n",
    "    if model is None:\n",
    "        model = get_model()\n",
    "    \n",
    "    if(verbose > 0):\n",
    "        print('Train...')\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_reader.ptb_iterator(train_data, batch_size, num_steps)):    \n",
    "#         x1 = np.zeros((dataset_size, num_steps, feat_len))\n",
    "#         for i in range(x1.shape[0]):\n",
    "#             for j in range(x1.shape[1]):\n",
    "#                 x1[i,j,x[i,j]] = 1\n",
    "\n",
    "#         y1 = y[:,-1]\n",
    "\n",
    "#         y1 = np.zeros((dataset_size, feat_len))\n",
    "#         for i in range(y1.shape[0]):\n",
    "#             y1[i,y[i,-1]] = 1\n",
    "#         print(x.shape)\n",
    "#         print(y.shape)\n",
    "#         x1, y1 = one_hot(x, y[:, -1])\n",
    "\n",
    "#         print(x1.shape)\n",
    "#         print(y1.shape)\n",
    "        \n",
    "#         x1_tf = np.reshape(x1, (batch_size, num_steps, feat_len))\n",
    "        y1_tf = np.reshape(y, (batch_size, num_steps, 1))\n",
    "        \n",
    "        \n",
    "        model.fit(x, y1_tf, epochs=10, verbose = verbose, batch_size=10)\n",
    "        loss, perplexity  = model.evaluate(x, y1_tf, verbose = verbose, batch_size=10)\n",
    "\n",
    "       \n",
    "#         if(step % 100 == 0 and verbose> 0):\n",
    "        if(step % 100 == 0 ):\n",
    "            print('\\n#',step+1, ' Perplexity: ', perplexity, 'CrossEnt Loss: ', loss)\n",
    "        \n",
    "        \n",
    "    if( not os.path.isdir('weights') ):\n",
    "        os.mkdir('weights')\n",
    "        \n",
    "    model.save('weights/my_model.h5')\n",
    "    \n",
    "#     model.save_weights('my_model_weights.h5')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(test_data, verbose = 0):\n",
    "\n",
    "    model =load_model('weights/my_model.h5')\n",
    "\n",
    "#     model = get_model()\n",
    "#     model.load_weights('my_model_weights.h5')\n",
    "    \n",
    "    acc = 0.0\n",
    "    siz = 0\n",
    "    perplexity = []\n",
    "    for step, (x, y) in enumerate(ptb_reader.ptb_iterator(test_data, 1000, num_steps)):\n",
    "        \n",
    "        x1, y1 = one_hot(x,y[:,-1])\n",
    "        print(x1.shape)\n",
    "        print(y1.shape)\n",
    "        \n",
    "\n",
    "        output = model.predict(x, verbose=verbose)\n",
    "        score  = model.evaluate(x, y1, verbose = 1, batch_size=10)\n",
    "\n",
    "\n",
    "        siz += 1 \n",
    "        \n",
    "        \n",
    "        print('')\n",
    "        print('Step: ',step+1, end='')\n",
    "        print(', Test accuracy:', accuracy )\n",
    "            \n",
    "        \n",
    "    print('Average Accuracy: ', acc/siz)\n",
    "    \n",
    "    return np.mean(perp_np)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     global feat_len\n",
    "#     FLAGS = tf.app.flags.FLAGS\n",
    "#     tf.app.flags.DEFINE_string('test', '../data/ptb.test.txt',\n",
    "#                        \"\"\"Path to file for testing \"\"\")\n",
    "\n",
    "# #     train_data, valid_data, test_data, vocab = ptb_reader.ptb_raw_data(\"../data\")\n",
    "#     train_data, feat_len = get_data('../data/ptb.train.txt')\n",
    "#     model = None\n",
    "#     model = train(train_data, verbose = 1, model = model)\n",
    "    \n",
    "#     test_data, _ = get_data(FLAGS.test)\n",
    "#     perp =test(test_data)\n",
    "#     print(perp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 1  Perplexity:  4.92719995743e+16 CrossEnt Loss:  121056472.0\n"
     ]
    }
   ],
   "source": [
    "#     train_data, valid_data, test_data, vocab = ptb_reader.ptb_raw_data(\"../data\")\n",
    "train_data, feat_len = get_data('../data/ptb.train.txt')\n",
    "model = None\n",
    "output,y = train(train_data, verbose = 0, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 10000)\n",
      "(20, 20)\n",
      "[  42  105 3213    6 1357   52   22  207   10   14   42    1 1266   36    6\n",
      " 1357    9 1149    5 2872]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[1,:])\n",
    "print(np.argmax(output[1,:,:], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data, _ = get_data('../data/ptb.test.txt')\n",
    "logit, label =test(test_data)\n",
    "# print(perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(logit, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a= np.asarray([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a[:,[2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
